{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0196ea63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#...... Tuning SVM with GridSearchCV ........\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# ------------------ LOAD DATA ------------------\n",
    "AttackFree = pd.read_csv(\"Attack_free new.csv\")[0:2369397]\n",
    "DoS = pd.read_csv(\"DoS_Attack_new.csv\")[0:656578]\n",
    "Fuzzy = pd.read_csv(\"Fuzzy_Attack_New.csv\")[0:591989]\n",
    "Impersonation = pd.read_csv(\"Impersonation_Attack_New.csv\")[0:995471]\n",
    "\n",
    "# ------------------ LABEL ENCODING ------------------\n",
    "def encode_dataframe(df):\n",
    "    le = LabelEncoder()\n",
    "    for col in df.columns:\n",
    "        df[col] = le.fit_transform(df[col].astype(str))\n",
    "    return df\n",
    "\n",
    "AttackFree = encode_dataframe(AttackFree)\n",
    "DoS = encode_dataframe(DoS)\n",
    "Fuzzy = encode_dataframe(Fuzzy)\n",
    "Impersonation = encode_dataframe(Impersonation)\n",
    "\n",
    "# ------------------ LABEL ASSIGNMENT ------------------\n",
    "label1 = [1] * len(AttackFree)\n",
    "label2 = [2] * len(DoS)\n",
    "label3 = [3] * len(Fuzzy)\n",
    "label4 = [4] * len(Impersonation)\n",
    "\n",
    "Dataset = np.concatenate((AttackFree.values, DoS.values, Fuzzy.values, Impersonation.values), axis=0)\n",
    "label = np.concatenate((label1, label2, label3, label4), axis=0)\n",
    "\n",
    "# ------------------ CLASS WEIGHTS ------------------\n",
    "class_weights_array = compute_class_weight(class_weight='balanced', classes=np.unique(label), y=label)\n",
    "class_weights_dict = dict(zip(np.unique(label), class_weights_array))\n",
    "print(\"Class Weights:\", class_weights_dict)\n",
    "\n",
    "# ------------------ TRAIN-TEST SPLIT + SCALING ------------------\n",
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(Dataset, label, test_size=0.25, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_Train = scaler.fit_transform(X_Train)\n",
    "X_Test = scaler.transform(X_Test)\n",
    "\n",
    "# ------------------ SVM TUNING ------------------\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.01, 0.001],\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=SVC(probability=True, class_weight=class_weights_dict),\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\n Tuning SVM...\")\n",
    "start_train = time.time()\n",
    "grid.fit(X_Train, Y_Train)\n",
    "end_train = time.time()\n",
    "print(f\"\\n Training Complete: {end_train - start_train:.2f} seconds\")\n",
    "print(\" Best Params:\", grid.best_params_)\n",
    "\n",
    "best_svm = grid.best_estimator_\n",
    "\n",
    "# ------------------ TESTING ------------------\n",
    "start_test = time.time()\n",
    "Y_Pred = best_svm.predict(X_Test)\n",
    "end_test = time.time()\n",
    "print(f\"\\n Testing Time: {end_test - start_test:.2f} seconds\")\n",
    "\n",
    "# ------------------ LATENCY ------------------\n",
    "sample_input = X_Test[:1000]\n",
    "start_inf = time.time()\n",
    "_ = best_svm.predict(sample_input)\n",
    "end_inf = time.time()\n",
    "print(f\"\\n Inference Time (1000 samples): {end_inf - start_inf:.4f} sec\")\n",
    "print(f\" Avg Latency per Sample: {(end_inf - start_inf)/1000 * 1000:.4f} ms\")\n",
    "\n",
    "# ------------------ METRICS ------------------\n",
    "print(\"\\n Classification Report (Tuned SVM):\")\n",
    "print(classification_report(Y_Test, Y_Pred, target_names=['AttackFree', 'DoS', 'Fuzzy', 'Impersonation']))\n",
    "\n",
    "print(f\" Accuracy: {accuracy_score(Y_Test, Y_Pred):.4f}\")\n",
    "print(f\" Precision: {precision_score(Y_Test, Y_Pred, average='micro'):.4f}\")\n",
    "print(f\" Recall: {recall_score(Y_Test, Y_Pred, average='micro'):.4f}\")\n",
    "print(f\" F1 Score: {f1_score(Y_Test, Y_Pred, average='micro'):.4f}\")\n",
    "print(f\" Cohen Kappa: {cohen_kappa_score(Y_Test, Y_Pred):.4f}\")\n",
    "\n",
    "# ------------------ CONFUSION MATRIX ------------------\n",
    "labels = ['AttackFree', 'DoS', 'Fuzzy', 'Impersonation']\n",
    "cm = confusion_matrix(Y_Test, Y_Pred)\n",
    "sns.heatmap(pd.DataFrame(cm, index=labels, columns=labels), annot=True, cmap='Blues', fmt='d')\n",
    "plt.title(\"Confusion Matrix - Tuned SVM\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ------------------ TPR & TNR ------------------\n",
    "TPR = []\n",
    "TNR = []\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    tp = cm[i, i]\n",
    "    fn = cm[i, :].sum() - tp\n",
    "    fp = cm[:, i].sum() - tp\n",
    "    tn = cm.sum() - (tp + fn + fp)\n",
    "\n",
    "    tpr = tp / (tp + fn) if (tp + fn) else 0\n",
    "    tnr = tn / (tn + fp) if (tn + fp) else 0\n",
    "\n",
    "    TPR.append(tpr)\n",
    "    TNR.append(tnr)\n",
    "\n",
    "print(\"\\n TPR (Recall) and TNR (Specificity) per class:\")\n",
    "for i, cls in enumerate(labels):\n",
    "    print(f\"{cls}:\\n  TPR = {TPR[i]:.4f}\\n  TNR = {TNR[i]:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
